---
title: "Text tokenization"
date: 2024-06-12T14:56:14+07:00
---

# Что такое токенизация
Пропущу часть, где объясняется зачем это делать, потому что причины скорее всего и так понятны.
Токенизация текста - это процесс разбиения документа на предложения, а предложения на отдельные слова, числа, знаки и прочее.
И тут напрашивается вопрос как это сделать? Разбивать текст на пробелы не всегда хороший вариант из возможных. Лучше делить текст на токены, где токены...
## Что такое токен
... это наименьшая логическая часть предложения, которая сама по себе несет какой-то смысл (поэтому и логическая).

# Как это сделать
Делать это надо исходя из задачи. 
## Токенизация для LLM с помощью BPE
Например, когда мы используем LLM, нам важно, чтобы каждый кусочек информации, подаваемый на вход, привносил какой-то смысл в результат предсказания, поэтому в таком случае токенизация может происходить посимвольно, по несколько символов. Так например можно использовать Byte pair encoding, для кодирования посимвольно, по группам символов и даже по самым частым словам. Тем самым на вход модели подается сжатая версия текста, со всеми вытекающими последствиями (меньше информации обрабатывать, какие-то объедененные части текста изучаются моделью как шаблонны с определенным смыслом)

В коде это можно использовать готовое решение - уже обученный токенизатор для ChatGPT из библиотеки tiktoken:
```python
!pip install tiktoken

import tiktoken

enc = tiktoken.get_encoding("o200k_base")
assert enc.decode(enc.encode("hello world")) == "hello world"

# To get the tokeniser corresponding to a specific model in the OpenAI API:
enc = tiktoken.encoding_for_model("gpt-4o")
```

Ссылка на исходный код библиотеки:
https://github.com/openai/tiktoken/tree/main

## Токенизация для задач попроще
Когда мы пытаемся решить задачу попроще, нам может быть достаточно классической обработки текста: удаление спец символов и стоп-слов, деление слов с помощью пробелов и приведение их в нормализованный вид (с помощью лемматизации или стемминга). Для этого можно воспользоваться специальными библиотеками и регулярными выражениями.

Я нашел обзорную статью что для чего используется, поэтому может быть интересно почитать про различные библиотеки для разных задач. Там также приводятся примеры использования и описываются преимущества и недостатки различных методов:
https://thepythoncode.com/article/tokenization-stemming-and-lemmatization-in-python#tokenization-by-splitting-the-sentence-by-whitespaces

# Источники

> https://www.youtube.com/watch?v=LZFriJ85BfM