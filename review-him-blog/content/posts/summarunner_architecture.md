---
title: "Summarunner architecture"
date: 2024-01-28T20:11:01+07:00
---
# О способах решения задачи саммаризации
Для решения задачи саммаризации используются техники, основанные на двух подходах: extractive и abstractive

## Extractive подходы
Выбирают важные имеющиеся предложения или отрывки из документа

## Abstractive подходы
Могут перефразировать документ, назвав важную информацию из него другими словами

----
Модель SummaRuNNer реализует именно Extractive подход, основанный на рекуррентных нейросетях. А подход для ее обучения позволяет *extractive* модели быть натренированной на abstractive summaries.

# Архитектура модели

Данная модель решает проблему саммаризации следующим образом: решается проблема бинарной классификации для каждого предложения в документе (брать предложение в саммари или не брать), учитывая классификацию предсказанную моделью для предыдущих предложений от текущего.

Для классификации последовательности в качестве основного "строительного" блока была взята модель [GRU]({{< ref "/posts/gru_model" >}} "GRU"). 
Архитектура модели состоит из двух-слойной двунаправленной GRU-RNN, которая выглядит следующим образом:
![Схема SummaRuNNer](/images/summarunner_01.png)

## Первый слой
На первом слое идет работа с последовательностями слов. Сначала преобразовываются слова в эмбединги, а затем две RNN: одна обрабатывает последовательность из эмбедингов слева направо, другая - справа налево, получают скрытое представление о предложениях.

## Второй слой
На втором слое модели идет обработка предложений. Происходит она следующим образом: векторы скрытых представлений слов после прохода слева направо конкатенируются с векторами скрытых представлений слов после прохода RNN обратно, а затем усредняются. Получившиеся средние векторы по предложениям снова подаются в двунаправленную RNN.
Затем получившиеся векторы скрытого представления по каждому предложению конкатенируются с такими же векторами с двух направлений прохода нейросети и берется среднее между ними. Таким образом мы получаем представление документа.

## Собираем все вместе
Полученное векторное представление документа, а также скрытые слои предложений затем используются для классификации каждого предложения. Учитывается смысл предложения, его позиция в документе, его смысл в документе, а также смысл других предложений до него. На основе всей этой информации принимается решение о том, брать ли предложение в саммари или не брать.

# Реализация в коде
Импорт необходимых модулей:
```python
import random
import math

import torch
random.seed(34)
```

Прежде всего напомним, как выглядит архитектура: ![Схема SummaRuNNer](/images/summarunner_01.png)

Как видим, начать следует с обработки слов: их перевода в векторное представление с помощью эмбеддингов. Для этой игрушечной реализации, я подготовлю эмбединги с помощью обычного рандома. Далее со всеми другими параметрами я буду делать то же самое для упрощения.

```python
# Предположим, что у нас есть 3 предложения, каждое из которых
# имеет по 3-4 слова

# пусть наш эмбединг переводит слова в вектора размерности 3
embed_dim = 3

Doc = []
for i in range(3):
    random_words_num = random.randint(3, 5)
    Doc.append(torch.randn((random_words_num, embed_dim)))
```

Взглянем снова на устройство нейросети и объявим два нижних слоя: для обработки последовательности слов в каждом предложении и для обработки векторных представлений предложений, полученных на предыдущем шаге. Оба слоя представляют собой двунаправленные рекуррентные нейросети GRU:
```python 
# Слой для обработки слов
gru_w = torch.nn.GRU(embed_dim, 5, bidirectional=True)

# Слой для обработки предложений
gru_s = torch.nn.GRU(10, 5, bidirectional=True)
```

Обрабатываем предложения по словам и по предложениям:
```python
# по словам
outputs_per_sent = torch.zeros((len(Doc), 5*2))
for i in range(len(Doc)):
    outputs, _ = gru_w(Doc[i])
    outputs_per_sent[i] = outputs.mean(axis=0)

# по предложениям
outputs_s, _ = gru_s(outputs_per_sent)
```

После этого мы можем получить векторное представление документа, для этого объявим еще пару параметров: 
```python
Wd = torch.randn((10, 10))
bd = torch.ones((1, 10))

# Векторное представление документа:
d = torch.tanh(torch.matmul(Wd, outputs_s.mean(axis=0)) + bd).squeeze(0)
```

А теперь самая интересная часть, для которой понадобятся еще несколько параметров:
```python
Wc = torch.randn((1, 10))
Ws = torch.randn((10, 10))
Wr = torch.randn((10, 10))
Wap = torch.randn((1, 1))
Wrp = torch.randn((1, 1))
bp = 0
```

Посмотрим на формулу, ради которой производились все предыдущие вычисления:

$P(y_j=1|h_j,s_j,d) = \sigma(W_c h_j \text{ -> content } \newline$
$+ h_j^T W_s d \text{ -> salience } \newline$
$- h_j^T W_r tanh(s_j) \text{ -> novelty } \newline$
$+ W_{ap} p_j^a \text{ -> abs. pos. imp. } \newline$
$+ W_{rp} p_j^r \text{ -> rel. pos. imp. } \newline$
$+ bp) \text{ -> bias term }$

Как видим, формула представляет функцию sigmoid'ы, которая берется от многочлена, каждый одночлен которого вносит в формулу свой смысл.

- Content - отражает смысл предложения *j*
- salience - важность предложения в документе
- novelty - избыточность предложения по отношению к предыдущим предложениям
- abs. pos. imp/real. pos.imp - абсолютное/относительное положение предложения в документе

Из предыдущей формулы нам остался непонятен только параметр $s_j$:
$$
s_j = \sum_{i=1}^{j-1} h_i P(y_i=1|h_i,s_i,d) 
$$
Теперь нам известны все части формулы, реализуем этот алгоритм в коде python:
```python
def proba_of_y(hj, sj, d, paj, prj):
    content = torch.matmul(Wc, hj).squeeze(0)
    salience = torch.matmul(hj.T, torch.matmul(Ws, d))
    novelty = torch.matmul(hj.T, torch.matmul(Wr, torch.tanh(sj)))
    abs_pos_imp = torch.matmul(Wap, paj.view(1)).squeeze(0)
    rel_pos_imp = torch.matmul(Wrp, prj.view(1)).squeeze(0)
    return torch.sigmoid(content + salience - novelty + abs_pos_imp + rel_pos_imp + bp)
```

Для определения относительного положения предложения в документе введем количество делений документа:
```python
num_of_divs = 3
```

Проходимся по каждому предложению и делаем предсказания:
```python
sj = torch.zeros(outputs_s.size(1))
doc_part_size = math.ceil(len(outputs_s) / num_of_divs)
for j in range(len(outputs_s)):
    hj = outputs_s[j]
    paj = torch.tensor(j).type(torch.float)
    prj = torch.tensor(j // doc_part_size).type(torch.float)
    j_pred = proba_of_y(hj, sj, d, paj, prj)
    sj += hj * j_pred
```

Вот собственно и все. Таким образом SummaRuNNer и делает предсказания.
Каждый предсказанный j_pred является вероятностью того, что предложение $j$ является частью саммари всего документа.

# Послесловие
Для этой статьи я остановлюсь на разборе архитектуры и ее работы. Вопрос ее обучения сводится к тому, как использовать абстрактные саммари для сравнения выбранных нейросетью предложений в качестве кандидатов на саммари. В статье предложено решение, а разбираться с этим в мои планы пока не входило.

# Источники:

> Ссылка на оригинальную статью: https://arxiv.org/pdf/1611.04230.pdf
