---
title: "RNN architecture"
date: 2024-01-14T18:44:47+07:00
---

# Где RNN используются

Технология рекуррентных нейронных сетей используется как основа в state of the art решениях в области языкового моделирования и генерации текста, распознавания речи, генерации описания изображений или видео тагинга.

Такой тип архитектуры нейронных сетей используется преимущественно в задачах определения паттернов в последовательности данных.
С помощью RNN можно обрабатывать такие данные как рукописный текст, геномы, текстовые или числовые временные серии (например котировки акций или показания датчиков). А также на вход такой модели можно подавать и части изображения для решения каких либо специфичных задач, как например описание изображения.

# Основной принцип работы RNN
## Общее описание принципа
Основной принцип работы RNN, в отличии от обычной архитектуры нейронной сети - Feedforward (многослойный персептрон), в том, как данные проходят по нейронной сети. В Feedforward нейронной сети данные проходят по ней без цикла, а RNN прогоняет информацию через себя несколько раз. Это позволяет расширить функциональность обычной нейросети и добавить к имеющейся информации на вход информацию о предыдущих проходах. Чтобы было понятно, входные данные об Xt будут также принимать во внимания информацию из входных данных об X0:t-1
Различия между сетями будут выглядеть следующим образом:
![Схема RNN](/images/rnn_model_05.png)
Количество скрытых слоев здесь может быть больше одного, так выглядит упрощение

## Архитектура модели
Первым делом мы должны инициализировать все необходимые матрицы для преобразования информации через них. А для этого инициализировать размерность входной информации и размерность скрытого слоя:

```python
n = 3 # number of samples
d = 5 # number of features
h = 7 # number of hidden units

X = torch.rand((n, d)) # input data
Wxh = torch.rand((d, h)) # input to hidden matrix
Whh = torch.rand((h, h)) # hidden state to hidden state matrix
bh = torch.rand((1, h)) # bias
```

Далее нам нужно получить матрицу скрытого слоя. Но для начала нам нужно инициализировать начальную матрицу скрытого слоя, чтобы осуществить эту операцию для X\[0\]:
```python
H_prev = torch.zeros((1, h))
```
Теперь считаем матрицу Ht по следующей формуле:
Ht = φh \* (Xt\*Wxh + Ht−1\*Whh + bh)
где:
φh - функция активации, обычно сигмоида или гиперболический тангенс, которая используется в обучении в алгоритме обратного распространения ошибки (backpropagation)

## Наглядное описание прохождения информации через RNN
```python 
def get_ht(X, Wxh, Whh, H_prev, bh):
    # X (1, d)
    # Wxh (d, h)
    # Whh (h, h)
    # H_prev (1, h)
    # bh (1, h)
    a = torch.matmul(X, Wxh) # (1, d) X (d, h) = (1, h)
    b = torch.matmul(H_prev, Whh) # (1, h) X (h, h) = (1, h)
    c = a + b + bh # (1, h)
    return c
Ht = torch.relu(get_ht(X[0], Wxh, Whh, H_prev, bh)) # torch.Size([1, 7]), H0
```

Полученное Ht это посчитанное скрытое представление для первого экземпляра (`X[0]`)

И поскольку Ht это накопительное скрытое состояние, то для остальных `Xt+1` и далее мы поочередно пробрасываем посчитанное на предыдущем шаге Ht (Из этой же переменной):
```python
Ht = torch.relu(get_ht(X[1], Wxh, Whh, Ht, bh)) # H1
Ht = torch.relu(get_ht(X[2], Wxh, Whh, Ht, bh)) # H2
```
В моих экспериментальных данных Ht для `X[2]` получилось равно `tensor([[36.6122, 26.2831, 52.2418, 44.8039, 34.2843, 24.4749, 44.1318]])`

Теперь для проверки правильности подсчетов, воспользуемся уже созданным блоком RNN и модуля torch.nn, загрузим в него наши обученные веса и прогоним наши данные через него, чтобы убедиться в правильности подсчетов:
```python
import torch.nn as nn
# Таким мобразом можно посмотреть название параметров модели, 
# чтобы дальше к ним можно было обращаться и менять их на наши. 
# Я этим воспользовался вне статьи:)
for k, v in rnn.named_parameters():
    print(k, v)

# Инициализируем пустую модель rnn из торча
rnn = nn.RNN(d, 7, nonlinearity='relu')

# меняем рандомные параметры модели на наши
# поскольку в формуле для умножения матрицы транспонируют, 
# тут мы загружаем наши веса именно транспонированные
# посмотреть формулу можно здесь: https://pytorch.org/docs/stable/generated/torch.nn.RNN.html
rnn.weight_ih_l0 = torch.nn.Parameter(Wxh.T)
rnn.weight_hh_l0 = torch.nn.Parameter(Whh.T)
# В формуле в rnn блоке из pytorch присутствует второй bias, 
# который не описан в оригинальной статье, поэтому присваеваем ему нули 
rnn.bias_ih_l0 = torch.nn.Parameter(torch.zeros(rnn.bias_hh_l0.size()))
rnn.bias_hh_l0 = torch.nn.Parameter(bh)

# прогоняем данные через rnn, используя наши X и H_prev, 
# и получаем output и hn
output, hn = rnn(X, H_prev)
```
Проверяем что содержится в hn и убеждаемся, что результаты одинаковые:
Ht = `tensor([[36.6122, 26.2831, 52.2418, 44.8039, 34.2843, 24.4749, 44.1318]])`
hn = `tensor([[36.6122, 26.2831, 52.2418, 44.8039, 34.2843, 24.4749, 44.1318]],grad_fn=<SqueezeBackward1>)`

Затем, чтобы получить предсказания `Ot` для последнего экземпляра, нам нужно как и в нейронной сети Feedforward архитектуры домножить получившийся `Ht` на веса `Who`, добавить `bias` и применить функцию активации. Эта последняя часть не отличается от обычного персептрона.

# Как обучать?

Обучение здесь осуществляется тем же методом обратного распространения ошибки, но так как очередность поданных элементов имеет значение, то и метод обучение здесь немного особенный. Он даже называется не просто Backpropagation, а Backpropagation through time (BPTT). 

## Backpropagation Through Time (BPTT)
Когда мы делает проход по нейронной сети для какой-то последовательности, на выходе мы получаем для каждого элемента в последовательности скрытое представление в виде вектора `Ht`, а также мы считаем на каждом шаге `Ot`, которое будет обозначать предсказания. 
Теперь мы можем использовать наши предсказания для составления нашей функции потерь. 
Обозначим функцию потерь как L(O, Y). Тогда значение этой функции потерь для какой-то последовательности будет равняться сумме результатов функций потерь по каждому элементу последовательности от предсказаний `Ot` и истинных значений `Yt`

![Формула функции потерь](/images/rnn_model_01.png)

В статье далее описываются математические вычисления градиента функции потерь по каждой матрице весов и все это упрощается до следующих формул:

1. ![Упрощение формул вычисления градиента](/images/rnn_model_02.png)
2. ![Упрощение формул вычисления градиента](/images/rnn_model_03.png)
3. ![Упрощение формул вычисления градиента](/images/rnn_model_04.png)

Из формул можно заметить, что нам следует сохранять матрицу $W_{hh}^k$ для подсчета функции потерь, но чем больше последовательность, тем больше будут значения этой матрицы, возведенные в степень $k$, поэтому на практике используется метод Truncated BPTT.

Суть этого метода заключается в том, чтобы ограничить количество суммируемых элементов для достижения подходящего размера степени, чтобы облегчить вычисления. При таком подходе следует проходить окном определенного размера по входным данным, а все что не попадает в это окно не считать. С помощью такого подхода мы также ограничиваем количество скрытых слоев сети для обучения.

### Реализация с pytorch
На практике BPTT с помощью pytorch может быть реализовано следующим образом:
```python
output, hiddens = rnn_model(inputs)
loss = loss_fn(output, targets)
loss.backward()
```
Потому что pytorch обо всем и сам прекрасно позаботится.
# Преимущества RNN
- На вход можно подавать любую последовательность данных
- Модель учитывает зависимости в последовательности
- Может генерировать векторное представление чего-либо - генератор эмбедингов из последовательности
- Является базовой архитектурой к более продвинутым моделям

# Недостатки RNN и их решения

## Затухание и взрыв градиента
Тут нечего добавить. Нейросеть рекуррентная, а значит, когда значения весов меньше единицы, умножение значений друг на друга будут давать меньшие значения. Поэтому чем больше будет последовательность данных при обучении, тем сильнее будет затухание градиента. 
По аналогии с предыдущей проблемой, когда значения весов будут больше единицы, значения градиента при их перемножении будет сильно расти. И чем больше будет последовательность данных при обучении, тем сильнее будет взрыв градиента.

Для решения этого недостатка была придумана более совершенная модель - LSTM.
Также недостаток взрыва градиента может быть решен с помощью метода clip_grad_norm_() в модуле utils pytorch. Этот метод использует метод клиппинга градиента, где значения вектора градиента  нормируются модулем этого вектора и умножаются на максимальное значение.
В коде это делается следующим образом:
```python
output, hiddens = rnn(inputs)
loss = loss_fn(output, targets)

loss.backward()
nn.utils.clip_grad_norm_(rnn.parameters(), 5)  # делаем ограничение максимальных значений градиентов, а затем optimizer step()
optimizer.step()
```
# Источники

> [Recurrent Neural Networks (RNNs), Clearly Explained!!!](https://www.youtube.com/watch?v=AsNTP8Kwu80&t=483s)
>
> [Рекуррентная нейронная сеть (RNN): виды, обучение и т.д.](https://neurohive.io/ru/osnovy-data-science/rekurrentnye-nejronnye-seti/)
>
> [Оригинальная статья](https://arxiv.org/pdf/1912.05911.pdf)
>
> https://www.kaggle.com/code/purvasingh/text-generation-via-rnn-and-lstms-pytorch

