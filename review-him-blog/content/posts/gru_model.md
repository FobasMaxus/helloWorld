---
title: "GRU - Gated Recurrent Unit"
date: 2024-01-15T22:58:45+07:00
---

# Что такое GRU
Расшифровка этой аббревиатуры выглядит так - The Gated Recurrent Unit. Это усовершенствованная архитектура нейронной сети, которая перекрывает некоторые недостатки таких архитектур, как RNN и LSTM. 
В отличие от RNN, эта архитектура может запоминать более далекие зависимости, а также в этой архитектуре в какой-то степени решена проблема затухания и взрыва градиента.
А в отличие от LSTM, эта архитектура имеет меньшее количество параметров, что повышает скорость ее обучения, но не сильно ухудшает качество предсказаний модели.

GRU помнит о важной информации и контексте благодаря двум "воротам", которые называются gates: *Reset gate* и *Update gate*.
# Описание архитектуры GRU
Внутри одной GRU ячейки происходит следующее:
## 1. На вход подается вектор скрытого представления $H_{t-1}$, а также информация о текущей записи $X_t$. 
В качестве первого вектора скрытого представления можно взять вектор состоящий из нолей

## 2. Вычисляется Update gate
Update gate (Z) вычисляется по следующей формуле:

$$
Z_t = \sigma(X_tW_{xz} + H_{t-1}W_{hz} + b_z)
$$
>В этой и следующих формулах символ $\sigma$ означает функцию sigmoid'ы
## 3. Вычисляется Reset gate
Точно такая же формула как на предыдущем шаге, только взяты другие веса
$$
R_t = \sigma(X_tW_{xr} + H_{t-1}W_{hr} + b_r)
$$
## 4. Вычисляется $\hat{H_t}$
$\hat{H_t}$ помогает понять, какую информацию из предыдущего вектора скрытого представления нужно взять во внимание, в соответствии с входной информацией.

Формула для вычисления $\hat{H_t}$ выглядит так:
$$
\hat{H_t} = tanh(X_tW_{xh} + (R_t \odot H_{t-1}) W_{hh} + b_h)
$$

>В этой и следующих формулах символ $\odot$ означает поэлементное умножение элементов матриц одинаковой размерности. Иначе эту операцию называют *Hadamard product* или *elementwise product*.
## 5. Вычисляется $H_t$

И наконец вычисляется $H_t$:
$$
H_t = Zt \odot H_{t-1} + (1 - Z_t) \odot \hat{H_t}
$$
# Как реализовать в коде

## Самостоятельно

Сначала обозначим матрицу входных данных, а также все необходимые параметры модели (для эксперимента генерируем их с помощью torch.randn()):
```python
import torch

h = 4  # number of hidden units
d = 5  # number of features in data
n = 3  # number of records in sequence

X = torch.randn((n, d))
H_prev = torch.zeros(h)

# nn weights
Whr = torch.randn((h, h))
Whz = torch.randn((h, h))
Whh = torch.randn((h, h))

Wxr = torch.randn((d, h))
Wxz = torch.randn((d, h))
Wxh = torch.randn((d, h))

br = torch.ones(h)
bz = torch.ones(h)
bh = torch.ones(h)
```

Затем пройдем по алгоритму. Для эксперимента возьмем только первый элемент матрицы X.
Выполним пункты 1, 2 и 3 в одной ячейке
```python
# reset gate
Rt = torch.sigmoid(torch.matmul(X[0], Wxr) + torch.matmul(H_prev, Whr) + br)

# update gate
Zt = torch.sigmoid(torch.matmul(X[0], Wxz) + torch.matmul(H_prev, Whz) + bz)
```

Затем вычисляем $\hat{H_t}$:
```python
H_hat_t = (torch.tanh(torch.matmul(X[0], Wxh) 
			  + torch.matmul((Rt * H_prev), Whh) 
			  + bh))
```

И после этого считаем скрытое представление первого слоя - $H_1$:
```python
H_1 = Zt * H_prev + (1 - Zt) * H_hat_t
```
Реализуем ту же самую логику, только c помощью Pytorch
## Pytorch
```python
gru = torch.nn.GRU(d, h)

# заменим веса сгенерированные по умолчанию на те, которые использовали мы
gru.weight_ih_l0.data[:4, :] = Wxr.T
gru.weight_ih_l0.data[4:8, :] = Wxz.T
gru.weight_ih_l0.data[8:12, :] = Wxh.T

gru.weight_hh_l0.data[:4, :] = Whr.T
gru.weight_hh_l0.data[4:8, :] = Whz.T
gru.weight_hh_l0.data[8:12, :] = Whh.T

gru.bias_ih_l0.data[:4] = br
gru.bias_ih_l0.data[4:8] = bz
gru.bias_ih_l0.data[8:12] = bh

# И обнулим вектор, который мы не использовали в нашей формуле
# подробнее можно почитать в документации:
# https://pytorch.org/docs/stable/generated/torch.nn.GRU.html
gru.bias_hh_l0.data = torch.zeros(gru.bias_hh_l0.data.size())
```

Теперь передаем в модель наши входные данные и получаем одинаковые результаты:
```python
output, hidden = gru(X[None, 0], H_prev[None, :])
print(H_1, hidden)  
# tensor([-0.1318, -0.2864,  0.2564, -0.0395]), 
# tensor([[-0.1318, -0.2864,  0.2564, -0.0395]])
```

# Как обучать?
О том, как обучать подобную рекуррентную нейросеть (о способе BPTT) можно почитать в статье про [RNN](/posts/rnn_model/)

# Преимущества и недостатки
## Преимущества
В том, что модель определяет важность информации исходя из контекста и может помнить о ней от начала и до конца последовательности. Множество параметров сети способно находить зависимости и учитывать их при дальнейших вычислениях. А также такая модель частично решает проблемы с затуханием градиента, поскольку при обратном распространении ошибки, градиенты имеют несколько обходных путей до параметров. 

## Недостатки
Говорят, что из-за большого количества обучаемых параметров в архитектуре сети, она, как и ее старший аналог LSTM, имеют склонность к переобучению, поэтому обучение нейросети необходимо делать с использованием дропаутов и нормализаций по батчам. А также следует контролировать переобучение, хорошо подобранной выборкой валидации.


# Источники

> [Illustrated Guide to LSTM's and GRU's: A step by step explanation](https://www.youtube.com/watch?v=8HyCNIVRbSU)
> 
> [Gated Recurrent Unit (GRU)](https://www.scaler.com/topics/deep-learning/gru-network/)
> 
> [10.2. Gated Recurrent Units (GRU)](https://d2l.ai/chapter_recurrent-modern/gru.html)

