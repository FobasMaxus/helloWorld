---
title: "Loss functions"
date: 2024-02-02T21:26:28+07:00
---

Функция потерь (Loss function) - функция, которая говорит, насколько предсказания модели далеки от правды. С помощью функции потерь, можно подстраивать параметры модели таким образом, чтобы минимизировать значения этой функции, тем самым увеличивая точность предсказаний модели. А в статистике функции потерь используются для оценки подобранных параметров функций.

Далее я разберу распространенные функции потерь в задачах классификации и регрессии, а затем приведу примеры их расчета и использования на практике.

# Функции потерь для регрессии

## Mean Square Error / Quadratic Loss / L2 Loss
Самая часто используемая функция потерь в задачах регрессии - это MSE loss.
Формула этой функции выглядит следующим образом:
$$
\text{MSE} = \dfrac{1}{n} \sum_{i=1}^n (\hat{Y}_i - Y_i)^2
$$
Эта функция потерь примечательна тем, что возводит отклонение от правильного ответа в квадрат, тем самым сильно штрафуя за большие отклонения. Из-за этого функция чувствительна к выбросам.
![график mse](/images/loss_03.png)
## Mean Absolute Error / L1 Loss
MAE - это вторая по частоте использования для задач регрессии.
Ее формула выглядит похоже на предыдущую, с некоторыми отличиями (квадрат поменялся на модуль):
$$
\text{MAE} = \dfrac{1}{n} \sum_{i=1}^n |\hat{Y}_i - Y_i|
$$
Эта функция меньше подвержена влиянию выбросов, в сравнении с MSE, поэтому если выбросов в данных много, следует воспользоваться ей.
![график mae](/images/loss_04.png)
## Huber Loss / Smooth Mean Absolute Error
Эта функция потерь объединяет в себе две предыдущие, влияние которых зависит от дополнительного параметра $\sigma$:
$$
L_\sigma(Y, \hat{Y})= 
\begin{cases}
\dfrac{1}{2}(Y-\hat{Y})^2&,& \text{for } |Y - \hat{Y}| \le \sigma \newline
\sigma \left(|Y - \hat{Y}| - \dfrac{1}{2} \sigma\right) &,&\text{ otherwise }
\end{cases}
$$
 Эта функция более устойчива к выбросам, но следует правильно подбирать параметр $\sigma$.
 ![график huber loss](/images/loss_05.png)
## Log-cosh Loss
Эта функция потерь представляет собой логарифм гиперболического косинуса ошибки предсказаний модели. Формула этой функции выглядит следующим образом (В английских обозначениях функция гиперболического косинуса выглядит как $cosh$, но в русских источниках ее обозначают как $ch$ - так написано в википедии:D):
$$
L(Y, \hat{Y}) = \sum_{i=1}^{n} log(cosh(\hat{Y_i} - Y_i))
$$

![график logcosh](/images/loss_06.png)
Так как я изучаю концепции в ML, а также вспоминаю высшую математику, по мере встречаемости терминов и функций в формулах, то у меня возник вопрос, а чем функция $cosh$ отличается от обычного $cos$.
Я загуглил картинки графиков этих функций, чтобы понять различия и вот что выдал мне гугол:
![график cosh](/images/loss_01.png)
Оказывается, гиперболические функции называются так не спроста. Они означают, что берется обычная тригонометрическая функция, но определяется она через гиперболу, а не через окружность.
Формула гиперболического косинуса выглядит следующим образом:
$$
\begin{array}{l}
cosh(x) &=& \frac{e^x + e^{-x}}{2} = \newline
&=& \frac{e^{2x} + 1}{2e^x} = \newline
&=& \frac{1 + e^{-2x}}{2e^{-x}}
\end{array}
$$
Функции потерь, благодаря своему виду ($log(cosh(x))$), для маленьких ошибок вернет примерно тоже что и функция $(x^2) / 2$, а для больших значений ошибки вернет $abs(x) - log(2)$. Получается, что эта функция похожа на MSE Loss, кроме того, что она будет не так сильна подвержена влиянию больших значений ошибки.
## Quantile Loss
Название этой функции подсказывает нам, что она используется для обучения модели, которая предсказывает значение квантиля.
## Что такое квантиль
Квантиль - это значение, которое делит заданный набор чисел таким образом что $\alpha * 100\%$ чисел - меньше, чем этот квантиль - значение, а $(1 - \alpha) * 100\%$ чисел больше, чем этот квантиль. Такое определение не просто понять, поэтому на этой картинке есть пример того, как это работает и что из этого квантиль.

Для примера, возьмем рандомный список отсортированных чисел в порядке возрастания. Поделим этот список на 4 части таким образом, чтобы каждая следующая точка отделяла на 25% больше элементов этого списка. Еще один важный момент - точка должна быть выбрана из уже имеющихся значений в нашем списке. Вот так должно это все выглядеть:
![визуализация квантилей](/images/loss_02.png)
На этой картинке мы видим, что первый квантиль равен 10, потому что он отделяет 25% данных слева и 75% данных справа. Следующий квантиль будет равен 19, потому что он отделяет 50% данных слева и 50% данных справа, а третий квантиль будет равен 26, по той же логике.

## Как работает quantile loss
Предположим у нас есть какая-то модель, которая как раз и предсказывает значение какого-нибудь квантиля.
Если мы намерены предсказывать третий квантиль, то это означает, что в 75% случаев ошибки наших предсказаний должны быть отрицательными, а в остальных 25% случаев положительными.
В этом мы убедимся посмотрев на формулу функции потерь:
$$
L_\sigma (Y, \hat{Y}) = 
\begin{cases}
\alpha(Y - \hat{Y}) &, \hat{Y} \le Y &\newline
(1-\alpha)(\hat{Y} - Y) &, \hat{Y} > Y &
\end{cases}
$$
Как видим в формуле присутствует параметр $\alpha$, который означает персентиль, который мы хотим предсказать. От этого параметра будет зависить то, насколько сильно мы будем штрафовать модель за предсказания до или после персентиля или одинаково в обе стороны. Если мы рассмотрим эту формулу с параметром $\alpha = 4$, то мы увидим, что модель в таком случае будет гораздо сильнее штрафовать если мы предсказали значение меньше, чем исходное значение. А предсказания больше исходного значения будут штрафоваться меньше.
![график quantile loss](/images/loss_07.png)
# Функции потерь для классификации
## Binary Cross-Entropy Loss / Log Loss
Самая распространенная функция потерь для решения проблемы классификации. 
Суть этой функции потерь в том, что ее значение уменьшается по мере того, как модель предсказывает значение приближенное к истинному.
Используется эта функция, когда модель возвращает значения между 0 и 1, а количество классов модели = 2. Это и означает бинарность в названии этой функции.

$$
L(Y, \hat{Y}) = -\frac{1}{n} \sum_{i=1}^{n} (Y_i log(\hat{Y_i}) + (1 - Y_i) log(1-\hat{Y_i}))
$$
![график bce loss](/images/loss_08.png)
Для мультиклассовой классификации, когда количество классов больше 2, используется следующая функция:
$$
L(Y, \hat{Y}) = -\frac{1}{n} \sum_{i=1}^n Y_i log(\hat{Y}_i)
$$
## Hinge Loss
Вторая распространенная функция потерь для решения задач классификации. Ее применяют для SVM моделей.
Ее формула выглядит следующим образом:
$$
L(Y, \hat{Y}) = max(0, 1 - Y * \hat{Y})
$$

Эта функция потерь штрафует неправильные предсказания, а также неуверенные правильные предсказания. 
При использовании этой функции для классификации в SVM используются истинные значения классов -1 и 1, поэтому для использования этой функции нужно убедиться, что истинные значения в данных соответствуют -1 и 1
![график hinge loss](/images/loss_09.png)
# Практика
В качестве практики я собираюсь взять пару функций для регрессии и одну для классификации.

## Начнем с функции Log Loss для классификации
Для упрощения использую библиотеку pytorch. Код самой функции выглядит следующим образом:
```python
def log_loss(y_true, y_pred):
    choosen_probas = y_pred.gather(1, y_true.view(-1, 1))
    log_probas = torch.log10(choosen_probas)
    return -torch.mean(y_true * log_probas)
```
В коде этой функции нет ничего сверхъестественного, кроме функции gather, которую мы применили к y_pred. В нашем случае данные в переменной y_true выглядят следующим образом: `[0, 1, 2, 1, ...]`, то есть означают номера классов, которые мы и пытаемся предсказать, а значения в переменной y_pred выглядят `[[0.5, 0.25, 0.25], [...], [...], [...], ...]`, то есть у нас для каждого входящего массива признаков на выходе 3 нейрона возвращают вероятности для каждого класса. 
Поэтому в данном случае функция .gather() принимает параметр в доль какой оси выполнять соответствие индексов массива y_true, элементам массива y_pred.
Если еще проще, мы видим, что первый элемент массива y_true у нас 0, поэтому мы берем из первого массива y_pred значение вероятности 0, которую нам вернул нейрон модели, для следующего элемента из массива y_true, мы берем в следующем массиве y_pred значение вероятности с индексом 1 и так далее.

Кстати, в pytorch уже есть готовая реализация этой функции CrossEntropyLoss(), которая находится в модуле torch.nn, работает по батчам, а также имеет несколько дополнительных параметров.
## Mean Square Error
Без лишних слов оставляю здесь код функции. Функция mean, а также операция вычитания массивов с предсказаниями и правильными ответами, делаются из расчета на то, что их типы - это тензоры из pytorch или массивы из numpy.
```python
def mse_loss(y_pred, y_true):
    return ((y_pred - y_true) ** 2).mean()
```

## Log-cosh Loss
В pytorch код функции выглядел бы так.
При работе с большими отклонениями могут возникать некоторые трудности с подсчетами, поскольку e в большой степени затрудняет эти самые подсчеты.
```python 
def log_cosh(y_pred, y_true):
    dif = y_pred - y_true
    cosh = (torch.exp(2 * dif) + 1) / (2 * torch.exp(dif))
    return torch.log(cosh).mean()
```
# Источники

> https://www.youtube.com/watch?v=eKIX8F6RP-g
>
> https://wiki.loginom.ru/articles/loss-function.html
>
> https://builtin.com/machine-learning/common-loss-functions
>
> https://en.wikipedia.org/wiki/Huber_loss
>
> https://en.wikipedia.org/wiki/Hyperbolic_functions